{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import cycle\n",
    "import csv\n",
    "\n",
    "flight_df = pd.read_csv(\"flight_data.csv\")\n",
    "\n",
    "# Processing discrete columns (frequency calculation)\n",
    "discrete_columns = ['from_airport_code', 'from_country', 'dest_airport_code', 'dest_country', 'stops']\n",
    "\n",
    "# Calculate frequencies for 'from_country' and 'dest_country'\n",
    "from_country_freq = flight_df['from_country'].value_counts()\n",
    "dest_country_freq = flight_df['dest_country'].value_counts()\n",
    "\n",
    "# Calculate frequencies for 'from_airport_code' and 'dest_airport_code'\n",
    "from_airport_freq = flight_df['from_airport_code'].value_counts()\n",
    "dest_airport_freq = flight_df['dest_airport_code'].value_counts()\n",
    "\n",
    "# Create mappings based on sorted frequencies\n",
    "from_country_mapping = {country: idx for idx, country in enumerate(from_country_freq.index)}\n",
    "dest_country_mapping = {country: idx for idx, country in enumerate(dest_country_freq.index)}\n",
    "\n",
    "from_airport_mapping = {airport: idx for idx, airport in enumerate(from_airport_freq.index)}\n",
    "dest_airport_mapping = {airport: idx for idx, airport in enumerate(dest_airport_freq.index)}\n",
    "\n",
    "# For 'stops', we can use factorize since it's only in one column\n",
    "flight_df['stops'], _ = pd.factorize(flight_df['stops'])\n",
    "\n",
    "# Number of unique values for each column\n",
    "dim_nums = [\n",
    "    flight_df['from_airport_code'].nunique(),\n",
    "    flight_df['from_country'].nunique(),\n",
    "    flight_df['dest_airport_code'].nunique(),\n",
    "    flight_df['dest_country'].nunique(),\n",
    "    len(pd.unique(flight_df['stops']))\n",
    "]\n",
    "\n",
    "# Apply the mappings to the DataFrame\n",
    "flight_df['from_airport_code'] = flight_df['from_airport_code'].map(from_airport_mapping)\n",
    "flight_df['from_country'] = flight_df['from_country'].map(from_country_mapping)\n",
    "flight_df['dest_airport_code'] = flight_df['dest_airport_code'].map(dest_airport_mapping)\n",
    "flight_df['dest_country'] = flight_df['dest_country'].map(dest_country_mapping)\n",
    "\n",
    "# Add a header row with dim_num values\n",
    "header = discrete_columns\n",
    "\n",
    "# Save the dim_nums and the DataFrame to a CSV file\n",
    "csv_filename_with_dim = 'flight_data_discrete_with_price_time.csv'\n",
    "with open(csv_filename_with_dim, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(discrete_columns + ['price'] + ['departure_time'] + ['arrival_time'])  # Write the header\n",
    "    for index, row in flight_df.iterrows():\n",
    "        writer.writerow(row[discrete_columns].tolist() + [row['price'], row['departure_time'], row['arrival_time']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import cycle\n",
    "\n",
    "def init_tuples_and_data_info(filename, primary_cols, secondary_cols, exclude_col_index):\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    # Apply preprocessing that includes type conversions and mappings - this function is dataset specific to preprocess your dataset\n",
    "    df = type_preproc(df)\n",
    "    #print(df.columns)\n",
    "\n",
    "    actual_columns = df.columns.tolist()\n",
    "    if max(primary_cols + [col for col in secondary_cols if col != -1]) >= len(actual_columns):\n",
    "        raise ValueError(\"One or more column indices are out of range. Please check the input indices.\")\n",
    "\n",
    "    for primary, secondary in zip(primary_cols, secondary_cols):\n",
    "        if secondary != -1 and primary != exclude_col_index:\n",
    "            primary_name = df.columns[primary]\n",
    "            secondary_name = df.columns[secondary]\n",
    "\n",
    "            # Ensure secondary values are non-negative before applying the transformation\n",
    "            df[secondary_name] = df[secondary_name].apply(lambda x: max(0, x))\n",
    "            # Combine primary and secondary into a new value in the primary column\n",
    "            df[primary_name] = df.apply(lambda x: int(f\"{int(x[primary_name]):02d}{int(x[secondary_name]):03d}\"), axis=1)\n",
    "    \n",
    "    # Drop secondary columns post-combination\n",
    "    droplist = [df.columns[secondary] for secondary in secondary_cols if secondary != -1 and secondary < len(df.columns)]\n",
    "    df.drop(columns=droplist, inplace=True)\n",
    "\n",
    "    #print(df.columns)\n",
    "    return df\n",
    "\n",
    "def type_preproc(df):\n",
    "    df['departure_time'] = pd.to_datetime(df['departure_time'])\n",
    "    df['arrival_time'] = pd.to_datetime(df['arrival_time'])\n",
    "\n",
    "    # Calculate flight duration and convert to integer using the ceil function\n",
    "    df['duration'] = (df['arrival_time'] - df['departure_time']).dt.total_seconds() / 3600  # Duration in hours\n",
    "    df['duration'] = np.ceil(df['duration']).astype(int)  # Ceil and convert to integer    \n",
    "\n",
    "    if 'departure_time' in df.columns:\n",
    "        df['departure_time'] = df['departure_time'].dt.hour.apply(map_time_interval)\n",
    "    if 'arrival_time' in df.columns:\n",
    "        df['arrival_time'] = df['arrival_time'].dt.hour.apply(map_time_interval)\n",
    "\n",
    "    df[['from_airport_code', 'from_country', 'dest_airport_code', 'dest_country', 'stops']] = df[['from_airport_code', 'from_country', 'dest_airport_code', 'dest_country', 'stops']].astype('int64')\n",
    "\n",
    "    return df\n",
    "\n",
    "def map_time_interval(hour):\n",
    "    # Map hours to time intervals\n",
    "    intervals = [4, 8, 12, 16, 20, 24]\n",
    "    for i, interval in enumerate(intervals):\n",
    "        if hour < interval:\n",
    "            return i\n",
    "    return 5  # Default case \n",
    "\n",
    "def apply_bucket_mapping(df, bucket_details, column_names):\n",
    "    for column in column_names:\n",
    "        # Initialize the bucket column with None\n",
    "        bucket_col_name = f\"{column}_bucket\"\n",
    "        df[bucket_col_name] = None\n",
    "        \n",
    "        # Get a series with the current column values\n",
    "        col_values = df[column]\n",
    "        \n",
    "        # A dictionary to track the number of tuples needed to be assigned to each bucket\n",
    "        bucket_counts = {}\n",
    "\n",
    "        # Prepare a mapping from each bucket to its indices\n",
    "        for bucket_index, values_in_bucket in enumerate(bucket_details[column]):\n",
    "            for value, count in values_in_bucket.items():\n",
    "                # Get indices where the column value matches 'value' and bucket is still unassigned\n",
    "                valid_indices = df[(col_values == value) & (df[bucket_col_name].isnull())].index[:count]\n",
    "                \n",
    "                # Assign these indices the current bucket index\n",
    "                if not valid_indices.empty:\n",
    "                    df.loc[valid_indices, bucket_col_name] = bucket_index\n",
    "                    \n",
    "                    # Update the counts in bucket_counts\n",
    "                    if value in bucket_counts:\n",
    "                        bucket_counts[value] += len(valid_indices)\n",
    "                    else:\n",
    "                        bucket_counts[value] = len(valid_indices)\n",
    "\n",
    "        # Optionally verify if all counts are matched (could be commented out for speed)\n",
    "        for value in bucket_counts.keys():\n",
    "            # Calculate expected count by summing all occurrences of 'value' across all buckets\n",
    "            expected_count = sum(values_in_bucket.get(value, 0) for values_in_bucket in bucket_details[column])\n",
    "            if bucket_counts[value] != expected_count:\n",
    "                print(f\"Warning: Mismatch in counts for value {value} in column {column}: expected {expected_count}, got {bucket_counts[value]}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def calculate_bucket_allocations(grouped_data, num_buckets):\n",
    "    total_tuples = grouped_data['freq'].sum()\n",
    "    ideal_tuples_per_bucket = math.ceil(total_tuples / num_buckets)\n",
    "    \n",
    "    # Initialize bucket_details as a list of dictionaries for each bucket\n",
    "    bucket_details = [{} for _ in range(num_buckets)]\n",
    "    bucket_allocations = {}  # To keep track of which buckets each value goes into\n",
    "    current_bucket = 0\n",
    "    remaining_space_in_bucket = ideal_tuples_per_bucket\n",
    "\n",
    "    for index, row in grouped_data.iterrows():\n",
    "        value = row['value']\n",
    "        freq = row['freq']\n",
    "        start_bucket = current_bucket  # Remember the starting bucket for this value\n",
    "\n",
    "        while freq > 0:\n",
    "            space_used = min(freq, remaining_space_in_bucket)\n",
    "            freq -= space_used\n",
    "            remaining_space_in_bucket -= space_used\n",
    "\n",
    "            # Track how many tuples of each value go into each bucket\n",
    "            if value in bucket_details[current_bucket]:\n",
    "                bucket_details[current_bucket][value] += space_used\n",
    "            else:\n",
    "                bucket_details[current_bucket][value] = space_used\n",
    "\n",
    "            if remaining_space_in_bucket == 0:\n",
    "                current_bucket += 1\n",
    "                if current_bucket < num_buckets:\n",
    "                    remaining_space_in_bucket = ideal_tuples_per_bucket\n",
    "\n",
    "        end_bucket = current_bucket - 1 if remaining_space_in_bucket == 0 else current_bucket\n",
    "        bucket_allocations[value] = (start_bucket, end_bucket)\n",
    "\n",
    "    return bucket_allocations, bucket_details\n",
    "\n",
    "# Integrate with existing workflow\n",
    "def process_columns(df, num_buckets, column_names):\n",
    "    all_allocations = {}\n",
    "    all_bucket_details = {}\n",
    "    for column in column_names:\n",
    "        grouped_data = df.groupby(column).size().reset_index(name='freq')\n",
    "        grouped_data.rename(columns={column: 'value'}, inplace=True)\n",
    "        bucket_allocations, bucket_details = calculate_bucket_allocations(grouped_data, num_buckets)\n",
    "        all_allocations[column] = bucket_allocations\n",
    "        all_bucket_details[column] = bucket_details\n",
    "\n",
    "    df = apply_bucket_mapping(df, all_bucket_details, column_names)\n",
    "    return df, all_allocations\n",
    "\n",
    "# Example usage\n",
    "primary_cols = [0, 1, 2, 3, 4]  \n",
    "secondary_cols = [-1, -1, -1, -1, -1]\n",
    "#secondary_cols = [6, 6, 7, 7, 8]\n",
    "#secondary_cols = [6, -1, 7, -1, 8]  # Make sure these indices are within the range of actual columns\n",
    "exclude_col_index = 5  # Exclude column 5 from processing\n",
    "\n",
    "try:\n",
    "    df = init_tuples_and_data_info('flight_data_discrete_with_price_time.csv', primary_cols, secondary_cols, exclude_col_index)\n",
    "    print(df.head())\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "column_names = ['from_airport_code', 'from_country', 'dest_airport_code', 'dest_country', 'stops']\n",
    "num_buckets = 128\n",
    "df_mapped, bucket_allocations = process_columns(df, num_buckets, column_names)\n",
    "df_mapped.to_csv('tuples_with_buckets.csv', index=False)\n",
    "print(\"Mapped DataFrame with bucket assignments has been saved to 'tuples_with_buckets.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "def plot_interactions(df, col1, col2, secondary1, secondary2, dataset_name, distribution_type, scale, bucket_suffix='_bucket', num_buckets=128):\n",
    "    # Determine the bucketed column names\n",
    "    bucket_col1 = col1 + bucket_suffix\n",
    "    bucket_col2 = col2 + bucket_suffix\n",
    "    \n",
    "    # Ensure the column names are in the DataFrame\n",
    "    if bucket_col1 not in df.columns or bucket_col2 not in df.columns:\n",
    "        raise ValueError(\"One or both bucket columns are not in the DataFrame\")\n",
    "    \n",
    "    # Prepare figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # Plot Bucket Interactions\n",
    "    interaction_matrix = np.zeros((num_buckets, num_buckets), dtype=int)\n",
    "    interaction_counts = df.groupby([bucket_col1, bucket_col2]).size().reset_index(name='counts')\n",
    "    for _, row in interaction_counts.iterrows():\n",
    "        bucket1 = int(row[bucket_col1])\n",
    "        bucket2 = int(row[bucket_col2])\n",
    "        count = row['counts']\n",
    "        if bucket1 < num_buckets and bucket2 < num_buckets:\n",
    "            interaction_matrix[bucket1, bucket2] += count\n",
    "    \n",
    "    sns.heatmap(interaction_matrix, cmap='hot', annot=False, ax=ax1)\n",
    "    title1 = f'Bucket Heatmap between {col1 + \" + \" + secondary1 if secondary1 else col1} and {col2 + \" + \" + secondary2 if secondary2 else col2}'\n",
    "    ax1.set_title(title1, pad=12)  \n",
    "    ax1.set_ylabel(bucket_col1)\n",
    "    ax1.set_xlabel(bucket_col2)\n",
    "\n",
    "    # Plot Value Interactions\n",
    "    interaction_matrix = pd.crosstab(df[col1], df[col2]).values\n",
    "    sns.heatmap(interaction_matrix, cmap='hot', annot=False, ax=ax2)\n",
    "    title2 = f'Value Heatmap between {col1 + \" + \" + secondary1 if secondary1 else col1} and {col2 + \" + \" + secondary2 if secondary2 else col2}'\n",
    "    ax2.set_title(title2, pad=12)  \n",
    "    ax2.set_ylabel(col1)\n",
    "    ax2.set_xlabel(col2)\n",
    "\n",
    "    # Add a main title for the page\n",
    "    main_title = f'Plots of {dataset_name} dataset showing {distribution_type} distribution in {scale} scale'\n",
    "    fig.suptitle(main_title, fontsize=16)  \n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust the layout to prevent overlap\n",
    "    plt.subplots_adjust(top=0.9, wspace=0.008)  # Adjust wspace to add space between subplots\n",
    "    return fig\n",
    "\n",
    "# Creating the PDF with plots for each pair of columns\n",
    "def generate_interaction_plots(df, columns, secondary_columns, dataset_name, distribution_type, scale, num_buckets=128):\n",
    "    with PdfPages('flight_freq_minmax_sec.pdf') as pdf:\n",
    "        for i in range(len(columns)):\n",
    "            for j in range(i + 1, len(columns)):\n",
    "                col1 = columns[i]\n",
    "                col2 = columns[j]\n",
    "                sec1 = secondary_columns[i]\n",
    "                sec2 = secondary_columns[j]\n",
    "                \n",
    "                fig = plot_interactions(df, col1, col2, sec1, sec2, dataset_name, distribution_type, scale, num_buckets=num_buckets)\n",
    "                pdf.savefig(fig)\n",
    "                plt.close(fig)\n",
    "\n",
    "# Usage example - secondary_columns should contain the names or None\n",
    "columns = ['from_airport_code', 'from_country', 'dest_airport_code', 'dest_country', 'stops']\n",
    "secondary_columns = ['departure_time', None, 'arrival_time', None, 'duration' ]  \n",
    "#secondary_columns = [None, None, None, None, None]\n",
    "dataset_name = \"Flight\"\n",
    "distribution_type = \"frequency\"\n",
    "scale = \"absolute\"\n",
    "generate_interaction_plots(df, columns, secondary_columns, dataset_name, distribution_type, scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.colors import TwoSlopeNorm, LinearSegmentedColormap\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "def plot_interactions_ideal(df, col1, col2, secondary1, secondary2, dataset_name, distribution_type, scale, bucket_suffix='_bucket', num_buckets=128):\n",
    "    # Determine the bucketed column names\n",
    "    bucket_col1 = col1 + bucket_suffix\n",
    "    bucket_col2 = col2 + bucket_suffix\n",
    "    \n",
    "    # Ensure the column names are in the DataFrame\n",
    "    if bucket_col1 not in df.columns or bucket_col2 not in df.columns:\n",
    "        raise ValueError(\"One or both bucket columns are not in the DataFrame\")\n",
    "    \n",
    "    # Prepare figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # Plot Bucket Interactions\n",
    "    interaction_matrix = np.zeros((num_buckets, num_buckets), dtype=int)\n",
    "    interaction_counts = df.groupby([bucket_col1, bucket_col2]).size().reset_index(name='counts')\n",
    "    for _, row in interaction_counts.iterrows():\n",
    "        bucket1 = int(row[bucket_col1])\n",
    "        bucket2 = int(row[bucket_col2])\n",
    "        count = row['counts']\n",
    "        if bucket1 < num_buckets and bucket2 < num_buckets:\n",
    "            interaction_matrix[bucket1, bucket2] += count\n",
    "\n",
    "    # Total counts to determine average frequency per bucket for normalization\n",
    "    total_counts = interaction_matrix.sum()\n",
    "    average_frequency_per_bucket = total_counts / (num_buckets ** 2)\n",
    "\n",
    "    # Custom colormap: green to black to red\n",
    "    cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", [\"green\", \"black\", \"red\"])\n",
    "    norm = TwoSlopeNorm(vmin=0, vcenter=average_frequency_per_bucket, vmax=2 * average_frequency_per_bucket)\n",
    "\n",
    "    sns.heatmap(interaction_matrix, cmap=cmap, norm=norm, annot=False, ax=ax1)\n",
    "    title1 = f'Bucket Heatmap between {col1 + \" + \" + secondary1 if secondary1 else col1} and {col2 + \" + \" + secondary2 if secondary2 else col2}'\n",
    "    ax1.set_title(title1, pad=12)\n",
    "    ax1.set_ylabel(bucket_col1)\n",
    "    ax1.set_xlabel(bucket_col2)\n",
    "\n",
    "    # Plot Value Interactions\n",
    "    interaction_matrix = pd.crosstab(df[col1], df[col2]).values\n",
    "    \n",
    "    # Calculate average frequency per value pair\n",
    "    unique_values_col1 = df[col1].unique()\n",
    "    unique_values_col2 = df[col2].unique()\n",
    "    total_counts_values = interaction_matrix.sum()\n",
    "    average_frequency_per_value_pair = total_counts_values / (len(unique_values_col1) * len(unique_values_col2))\n",
    "\n",
    "    norm = TwoSlopeNorm(vmin=0, vcenter=average_frequency_per_value_pair, vmax=2 * average_frequency_per_value_pair)\n",
    "\n",
    "    sns.heatmap(interaction_matrix, cmap=cmap, norm=norm, annot=False, ax=ax2)\n",
    "    title2 = f'Value Heatmap between {col1 + \" + \" + secondary1 if secondary1 else col1} and {col2 + \" + \" + secondary2 if secondary2 else col2}'\n",
    "    ax2.set_title(title2, pad=12)\n",
    "    ax2.set_ylabel(col1)\n",
    "    ax2.set_xlabel(col2)\n",
    "\n",
    "    # Add a main title for the page\n",
    "    main_title = f'Plots of {dataset_name} dataset showing {distribution_type} distribution in {scale} scale'\n",
    "    fig.suptitle(main_title, fontsize=16)\n",
    "    # Print the average frequencies at the bottom of the page\n",
    "    fig.text(0.5, 0.01, f'Average Frequency per Bucket: {average_frequency_per_bucket:.2f}', ha='center', fontsize=12)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.93])  \n",
    "    plt.subplots_adjust(top=0.9, wspace=0.011)  \n",
    "    return fig\n",
    "\n",
    "# Creating the PDF with plots for each pair of columns\n",
    "def generate_interaction_plots_ideal(df, columns, secondary_columns, dataset_name, distribution_type, scale, num_buckets=128):\n",
    "    with PdfPages('flight_freq_ideal_sec.pdf') as pdf:\n",
    "        for i in range(len(columns)):\n",
    "            for j in range(i + 1, len(columns)):\n",
    "                col1 = columns[i]\n",
    "                col2 = columns[j]\n",
    "                sec1 = secondary_columns[i]\n",
    "                sec2 = secondary_columns[j]\n",
    "                \n",
    "                fig = plot_interactions_ideal(df, col1, col2, sec1, sec2, dataset_name, distribution_type, scale, num_buckets=num_buckets)\n",
    "                pdf.savefig(fig)\n",
    "                plt.close(fig)\n",
    "\n",
    "# Usage example - secondary_columns should contain the names or None\n",
    "columns = ['from_airport_code', 'from_country', 'dest_airport_code', 'dest_country', 'stops']\n",
    "secondary_columns = ['departure_time', None, 'arrival_time', None, 'duration'] \n",
    "# secondary_columns = [None, None, None, None, None]\n",
    "dataset_name = \"Flight\"\n",
    "distribution_type = \"frequency\"\n",
    "scale = \"normalized\"\n",
    "generate_interaction_plots_ideal(df, columns, secondary_columns, dataset_name, distribution_type, scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "def plot_interactions_price(df, col1, col2, secondary1, secondary2, dataset_name, distribution_type, scale, bucket_suffix='_bucket', num_buckets=128):\n",
    "    # Determine the bucketed column names\n",
    "    bucket_col1 = col1 + bucket_suffix\n",
    "    bucket_col2 = col2 + bucket_suffix\n",
    "    \n",
    "    # Ensure the column names are in the DataFrame\n",
    "    if bucket_col1 not in df.columns or bucket_col2 not in df.columns:\n",
    "        raise ValueError(\"One or both bucket columns are not in the DataFrame\")\n",
    "    \n",
    "    # Prepare figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # Plot Bucket Interactions\n",
    "    interaction_matrix = np.zeros((num_buckets, num_buckets), dtype=float)\n",
    "    interaction_sums = df.groupby([bucket_col1, bucket_col2])['price'].sum().reset_index(name='total_price')\n",
    "    for _, row in interaction_sums.iterrows():\n",
    "        bucket1 = int(row[bucket_col1])\n",
    "        bucket2 = int(row[bucket_col2])\n",
    "        total_price = row['total_price']\n",
    "        if bucket1 < num_buckets and bucket2 < num_buckets:\n",
    "            interaction_matrix[bucket1, bucket2] += total_price\n",
    "    \n",
    "    sns.heatmap(interaction_matrix, cmap='hot', annot=False, ax=ax1)\n",
    "    title1 = f'Bucket Heatmap between {col1 + \" + \" + secondary1 if secondary1 else col1} and {col2 + \" + \" + secondary2 if secondary2 else col2}'\n",
    "    ax1.set_title(title1, pad=12)  \n",
    "    ax1.set_ylabel(bucket_col1)\n",
    "    ax1.set_xlabel(bucket_col2)\n",
    "\n",
    "    # Plot Value Interactions\n",
    "    interaction_matrix = pd.crosstab(df[col1], df[col2], values=df['price'], aggfunc='sum').fillna(0).values\n",
    "    sns.heatmap(interaction_matrix, cmap='hot', annot=False, ax=ax2)\n",
    "    title2 = f'Value Heatmap between {col1 + \" + \" + secondary1 if secondary1 else col1} and {col2 + \" + \" + secondary2 if secondary2 else col2}'\n",
    "    ax2.set_title(title2, pad=12)  \n",
    "    ax2.set_ylabel(col1)\n",
    "    ax2.set_xlabel(col2)\n",
    "\n",
    "    # Add a main title for the page\n",
    "    main_title = f'Plots of {dataset_name} dataset showing {distribution_type} distribution in {scale} scale'\n",
    "    fig.suptitle(main_title, fontsize=16)  \n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96]) \n",
    "    plt.subplots_adjust(top=0.9, wspace=0.008)  \n",
    "    return fig\n",
    "\n",
    "# Creating the PDF with plots for each pair of columns\n",
    "def generate_interaction_plots_price(df, columns, secondary_columns, dataset_name, distribution_type, scale, num_buckets=128):\n",
    "    with PdfPages('flight_price_minmax_sec.pdf') as pdf:\n",
    "        for i in range(len(columns)):\n",
    "            for j in range(i + 1, len(columns)):\n",
    "                col1 = columns[i]\n",
    "                col2 = columns[j]\n",
    "                sec1 = secondary_columns[i]\n",
    "                sec2 = secondary_columns[j]\n",
    "                \n",
    "                fig = plot_interactions_price(df, col1, col2, sec1, sec2, dataset_name, distribution_type, scale, num_buckets=num_buckets)\n",
    "                pdf.savefig(fig)\n",
    "                plt.close(fig)\n",
    "\n",
    "# Usage example - secondary_columns should contain the names or None\n",
    "columns = ['from_airport_code', 'from_country', 'dest_airport_code', 'dest_country', 'stops']\n",
    "secondary_columns = ['departure_time', None, 'arrival_time', None, 'duration']  \n",
    "# secondary_columns = [None, None, None, None, None]\n",
    "dataset_name = \"Flight\"\n",
    "distribution_type = \"price\"\n",
    "scale = \"absolute\"\n",
    "generate_interaction_plots_price(df, columns, secondary_columns, dataset_name, distribution_type, scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.colors import TwoSlopeNorm, LinearSegmentedColormap\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "def plot_interactions_price_ideal(df, col1, col2, secondary1, secondary2, dataset_name, distribution_type, scale, bucket_suffix='_bucket', num_buckets=128):\n",
    "    # Determine the bucketed column names\n",
    "    bucket_col1 = col1 + bucket_suffix\n",
    "    bucket_col2 = col2 + bucket_suffix\n",
    "    \n",
    "    # Ensure the column names are in the DataFrame\n",
    "    if bucket_col1 not in df.columns or bucket_col2 not in df.columns:\n",
    "        raise ValueError(\"One or both bucket columns are not in the DataFrame\")\n",
    "    \n",
    "    # Prepare figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # Plot Bucket Interactions\n",
    "    interaction_matrix = np.zeros((num_buckets, num_buckets), dtype=float)\n",
    "    interaction_sums = df.groupby([bucket_col1, bucket_col2])['price'].sum().reset_index(name='total_price')\n",
    "    for _, row in interaction_sums.iterrows():\n",
    "        bucket1 = int(row[bucket_col1])\n",
    "        bucket2 = int(row[bucket_col2])\n",
    "        total_price = row['total_price']\n",
    "        if bucket1 < num_buckets and bucket2 < num_buckets:\n",
    "            interaction_matrix[bucket1, bucket2] += total_price\n",
    "\n",
    "    # Calculate average price per bucket\n",
    "    total_price = interaction_matrix.sum()\n",
    "    average_price = total_price / (num_buckets ** 2)\n",
    "\n",
    "    # Custom colormap: green to black to red\n",
    "    cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", [\"green\", \"black\", \"red\"])\n",
    "    norm = TwoSlopeNorm(vmin=0, vcenter=average_price, vmax=2 * average_price)\n",
    "\n",
    "    sns.heatmap(interaction_matrix, cmap=cmap, norm=norm, annot=False, ax=ax1)\n",
    "    title1 = f'Bucket Price Heatmap between {col1 + \" + \" + secondary1 if secondary1 else col1} and {col2 + \" + \" + secondary2 if secondary2 else col2}'\n",
    "    ax1.set_title(title1, pad=12)  \n",
    "    ax1.set_xlabel(bucket_col2)\n",
    "    ax1.set_ylabel(bucket_col1)\n",
    "\n",
    "    # Plot Value Interactions (Summing up price)\n",
    "    unique_values_col1 = df[col1].unique()\n",
    "    unique_values_col2 = df[col2].unique()\n",
    "    interaction_matrix = pd.DataFrame(index=unique_values_col1, columns=unique_values_col2, data=0.0)\n",
    "    interaction_sums = df.groupby([col1, col2])['price'].sum().reset_index(name='total_price')\n",
    "    for _, row in interaction_sums.iterrows():\n",
    "        value1 = row[col1]\n",
    "        value2 = row[col2]\n",
    "        total_price = row['total_price']\n",
    "        interaction_matrix.loc[value1, value2] = total_price\n",
    "\n",
    "    # Calculate average price per value pair\n",
    "    total_price_values = interaction_matrix.values.sum()\n",
    "    average_price_per_value_pair = total_price_values / (len(unique_values_col1) * len(unique_values_col2))\n",
    "\n",
    "    norm = TwoSlopeNorm(vmin=0, vcenter=average_price_per_value_pair, vmax=2 * average_price_per_value_pair)\n",
    "\n",
    "    # Mapping the unique values to indices\n",
    "    index_mapping_col1 = {val: idx+1 for idx, val in enumerate(unique_values_col1)}\n",
    "    index_mapping_col2 = {val: idx+1 for idx, val in enumerate(unique_values_col2)}\n",
    "\n",
    "    # Create a new DataFrame for the interaction matrix with indices\n",
    "    interaction_matrix_indexed = interaction_matrix.rename(index=index_mapping_col1, columns=index_mapping_col2)\n",
    "\n",
    "    sns.heatmap(interaction_matrix_indexed, cmap=cmap, norm=norm, annot=False, ax=ax2)\n",
    "    title2 = f'Value Price Heatmap between {col1 + \" + \" + secondary1 if secondary1 else col1} and {col2 + \" + \" + secondary2 if secondary2 else col2}'\n",
    "    ax2.set_title(title2, pad=12)  \n",
    "    ax2.set_xlabel(f'{col2} (Indices)')\n",
    "    ax2.set_ylabel(f'{col1} (Indices)')\n",
    "\n",
    "    # Add a main title for the page\n",
    "    main_title = f'Plots of {dataset_name} dataset showing {distribution_type} distribution in {scale} scale'\n",
    "    fig.suptitle(main_title, fontsize=16)  \n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.93])  \n",
    "    plt.subplots_adjust(top=0.9, wspace=0.011)  \n",
    "\n",
    "    # Print the average price at the bottom of the page\n",
    "    fig.text(0.5, 0.01, f'Average Price per Bucket: {average_price:.2f}', ha='center', fontsize=12)\n",
    "    return fig\n",
    "\n",
    "# Creating the PDF with plots for each pair of columns\n",
    "def generate_interaction_plots_price_ideal(df, columns, secondary_columns, dataset_name, distribution_type, scale, num_buckets=128):\n",
    "    with PdfPages('flight_price_ideal.pdf') as pdf:\n",
    "        for i in range(len(columns)):\n",
    "            for j in range(i + 1, len(columns)):\n",
    "                col1 = columns[i]\n",
    "                col2 = columns[j]\n",
    "                sec1 = secondary_columns[i]\n",
    "                sec2 = secondary_columns[j]\n",
    "                \n",
    "                fig = plot_interactions_price_ideal(df, col1, col2, sec1, sec2, dataset_name, distribution_type, scale, num_buckets=num_buckets)\n",
    "                pdf.savefig(fig)\n",
    "                plt.close(fig)\n",
    "\n",
    "# Usage example - secondary_columns should contain the names or None\n",
    "columns = ['from_airport_code', 'from_country', 'dest_airport_code', 'dest_country', 'stops']\n",
    "# secondary_columns = ['departure_time', None, 'arrival_time', None, 'duration']  \n",
    "secondary_columns = [None, None, None, None, None]\n",
    "dataset_name = \"Flight\"\n",
    "distribution_type = \"price\"\n",
    "scale = \"normalized\"\n",
    "generate_interaction_plots_price_ideal(df, columns, secondary_columns, dataset_name, distribution_type, scale)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

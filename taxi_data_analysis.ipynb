{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime',\n",
      "       'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag',\n",
      "       'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra',\n",
      "       'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge',\n",
      "       'total_amount', 'congestion_surcharge'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "taxi_df = pd.read_csv(\"taxi/yellow_tripdata_2019-01.csv\")\n",
    "print(taxi_df.columns)\n",
    "\n",
    "columns_to_keep = [\n",
    "    'passenger_count', 'trip_distance', 'PULocationID', 'DOLocationID', \n",
    "    'payment_type', 'fare_amount', 'tpep_pickup_datetime', 'tpep_dropoff_datetime'\n",
    "]\n",
    "\n",
    "# Select these columns from the DataFrame\n",
    "df_selected = taxi_df[columns_to_keep]\n",
    "\n",
    "# Save the selected columns to a new CSV file\n",
    "df_selected.to_csv('taxi_jan_with_sec.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   passenger_count  trip_distance  PULocationID  DOLocationID  payment_type  \\\n",
      "0             1000            1.5           151           239          1000   \n",
      "1             1000            2.6           239           246          1001   \n",
      "2             3013            0.0           236           236          1013   \n",
      "3             5015            0.0           193           193          2015   \n",
      "4             5015            0.0           193           193          2015   \n",
      "\n",
      "   fare_amount  \n",
      "0          7.0  \n",
      "1         14.0  \n",
      "2          4.5  \n",
      "3          3.5  \n",
      "4         52.0  \n",
      "Processing column: passenger_count\n",
      "Processing column: PULocationID\n",
      "Processing column: DOLocationID\n",
      "Processing column: payment_type\n",
      "Mapped DataFrame with bucket assignments has been saved to 'tuples_with_buckets_taxi_sec.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def init_tuples_and_data_info(filename, primary_cols, secondary_cols, exclude_col_index):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    # Apply preprocessing that includes type conversions and mappings\n",
    "    df = type_preproc(df)\n",
    "    actual_columns = df.columns.tolist()\n",
    "    if max(primary_cols + [col for col in secondary_cols if col != -1]) >= len(actual_columns):\n",
    "        raise ValueError(\"One or more column indices are out of range. Please check the input indices.\")\n",
    "\n",
    "    for primary, secondary in zip(primary_cols, secondary_cols):\n",
    "        if secondary != -1 and primary != exclude_col_index:\n",
    "            primary_name = df.columns[primary]\n",
    "            secondary_name = df.columns[secondary]\n",
    "\n",
    "            # Ensure primary and secondary values are integers before applying the transformation\n",
    "            df[primary_name] = df[primary_name].astype(int)\n",
    "            df[secondary_name] = df[secondary_name].apply(lambda x: max(0, int(x)))\n",
    "            # Combine primary and secondary into a new value in the primary column\n",
    "            df[primary_name] = df.apply(lambda x: int(f\"{int(x[primary_name]):02d}{int(x[secondary_name]):03d}\"), axis=1)\n",
    "    \n",
    "    # Drop secondary columns post-combination\n",
    "    droplist = [df.columns[secondary] for secondary in secondary_cols if secondary != -1 and secondary < len(df.columns)]\n",
    "    df.drop(columns=droplist, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def type_preproc(df):\n",
    "    df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])\n",
    "    df['tpep_dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'])   \n",
    "\n",
    "    if 'tpep_pickup_datetime' in df.columns:\n",
    "        df['tpep_pickup_datetime'] = df['tpep_pickup_datetime'].dt.hour.apply(map_time_interval)\n",
    "    if 'tpep_dropoff_datetime' in df.columns:\n",
    "        df['tpep_dropoff_datetime'] = df['tpep_dropoff_datetime'].dt.hour.apply(map_time_interval)\n",
    "        \n",
    "    df[['passenger_count', 'PULocationID', 'DOLocationID', 'payment_type']] = df[['passenger_count', 'PULocationID', 'DOLocationID', 'payment_type']].astype('int64')\n",
    "    return df\n",
    "\n",
    "def map_time_interval(hour):\n",
    "    # Map hours to time intervals\n",
    "    return hour // 1\n",
    "\n",
    "def apply_bucket_mapping(df, bucket_details, column_names):\n",
    "    for column in column_names:\n",
    "        bucket_col_name = f\"{column}_bucket\"\n",
    "        df[bucket_col_name] = None\n",
    "        \n",
    "        col_values = df[column]\n",
    "        bucket_counts = {}\n",
    "\n",
    "        for bucket_index, values_in_bucket in enumerate(bucket_details[column]):\n",
    "            for value, count in values_in_bucket.items():\n",
    "                valid_indices = df[(col_values == value) & (df[bucket_col_name].isnull())].index[:count]\n",
    "                if not valid_indices.empty:\n",
    "                    df.loc[valid_indices, bucket_col_name] = bucket_index\n",
    "                    bucket_counts[value] = bucket_counts.get(value, 0) + len(valid_indices)\n",
    "\n",
    "        for value in bucket_counts.keys():\n",
    "            expected_count = sum(values_in_bucket.get(value, 0) for values_in_bucket in bucket_details[column])\n",
    "            if bucket_counts[value] != expected_count:\n",
    "                print(f\"Warning: Mismatch in counts for value {value} in column {column}: expected {expected_count}, got {bucket_counts[value]}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_bucket_allocations(grouped_data, num_buckets):\n",
    "    total_tuples = grouped_data['freq'].sum()\n",
    "    ideal_tuples_per_bucket = math.ceil(total_tuples / num_buckets)\n",
    "    \n",
    "    bucket_details = [{} for _ in range(num_buckets)]\n",
    "    bucket_allocations = {}\n",
    "    current_bucket = 0\n",
    "    remaining_space_in_bucket = ideal_tuples_per_bucket\n",
    "\n",
    "    for _, row in grouped_data.iterrows():\n",
    "        value = row['value']\n",
    "        freq = row['freq']\n",
    "        start_bucket = current_bucket\n",
    "\n",
    "        while freq > 0:\n",
    "            space_used = min(freq, remaining_space_in_bucket)\n",
    "            freq -= space_used\n",
    "            remaining_space_in_bucket -= space_used\n",
    "\n",
    "            if value in bucket_details[current_bucket]:\n",
    "                bucket_details[current_bucket][value] += space_used\n",
    "            else:\n",
    "                bucket_details[current_bucket][value] = space_used\n",
    "\n",
    "            if remaining_space_in_bucket == 0:\n",
    "                current_bucket += 1\n",
    "                remaining_space_in_bucket = ideal_tuples_per_bucket if current_bucket < num_buckets else 0\n",
    "\n",
    "        end_bucket = current_bucket - 1 if remaining_space_in_bucket == 0 else current_bucket\n",
    "        bucket_allocations[value] = (start_bucket, end_bucket)\n",
    "\n",
    "    return bucket_allocations, bucket_details\n",
    "\n",
    "def process_columns(df, num_buckets, column_names):\n",
    "    all_allocations = {}\n",
    "    all_bucket_details = {}\n",
    "    for column in column_names:\n",
    "        print(f\"Processing column: {column}\")\n",
    "        grouped_data = df.groupby(column).size().reset_index(name='freq')\n",
    "        grouped_data.rename(columns={column: 'value'}, inplace=True)\n",
    "        bucket_allocations, bucket_details = calculate_bucket_allocations(grouped_data, num_buckets)\n",
    "        all_allocations[column] = bucket_allocations\n",
    "        all_bucket_details[column] = bucket_details\n",
    "\n",
    "    df = apply_bucket_mapping(df, all_bucket_details, column_names)\n",
    "    return df, all_allocations\n",
    "\n",
    "def allocate_buckets_continuous(df, continuous_columns, num_buckets):\n",
    "    total_tuples = len(df)\n",
    "    ideal_tuples_per_bucket = math.ceil(total_tuples / num_buckets)\n",
    "    \n",
    "    all_bucket_details = {}\n",
    "\n",
    "    for column in continuous_columns:\n",
    "        df_sorted = df.sort_values(by=column).reset_index(drop=True)\n",
    "        \n",
    "        bucket_details = [{} for _ in range(num_buckets)]\n",
    "        current_bucket = 0\n",
    "        remaining_space_in_bucket = ideal_tuples_per_bucket\n",
    "        \n",
    "        for i in range(total_tuples):\n",
    "            value = df_sorted.at[i, column]\n",
    "            if value in bucket_details[current_bucket]:\n",
    "                bucket_details[current_bucket][value] += 1\n",
    "            else:\n",
    "                bucket_details[current_bucket][value] = 1\n",
    "            \n",
    "            remaining_space_in_bucket -= 1\n",
    "            if remaining_space_in_bucket == 0:\n",
    "                current_bucket += 1\n",
    "                remaining_space_in_bucket = ideal_tuples_per_bucket if current_bucket < num_buckets else 0\n",
    "        \n",
    "        all_bucket_details[column] = bucket_details\n",
    "    \n",
    "    df = apply_bucket_mapping(df, all_bucket_details, continuous_columns)\n",
    "    return df, all_bucket_details\n",
    "\n",
    "# Example usage\n",
    "primary_cols = [0, 2, 3, 4]\n",
    "secondary_cols = [6, -1, -1, 7]\n",
    "exclude_col_index = 5  # Exclude column 5 from processing\n",
    "\n",
    "df = init_tuples_and_data_info('taxi_jan_with_sec.csv', primary_cols, secondary_cols, exclude_col_index)\n",
    "print(df.head())\n",
    "\n",
    "continuous_columns = ['trip_distance']\n",
    "discrete_columns = ['passenger_count', 'PULocationID', 'DOLocationID', 'payment_type']\n",
    "num_buckets = 128\n",
    "\n",
    "df_mapped_discrete, bucket_allocations_discrete = process_columns(df, num_buckets, discrete_columns)\n",
    "df_mapped_continuous, bucket_allocations_continuous = allocate_buckets_continuous(df, continuous_columns, num_buckets)\n",
    "\n",
    "# Combine discrete and continuous bucket allocations\n",
    "df_mapped = df_mapped_discrete.copy()\n",
    "for col in continuous_columns:\n",
    "    df_mapped[f\"{col}_bucket\"] = df_mapped_continuous[f\"{col}_bucket\"]\n",
    "\n",
    "df_mapped.to_csv('tuples_with_buckets_taxi_sec.csv', index=False)\n",
    "print(\"Mapped DataFrame with bucket assignments has been saved to 'tuples_with_buckets_taxi_sec.csv'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "def sanity_check_row_and_col_buckets(interaction_matrix):\n",
    "    # Print column sums and row sums of the interaction matrix\n",
    "    x_axis_sums = interaction_matrix.sum(axis=0)  # Column sums\n",
    "    y_axis_sums = interaction_matrix.sum(axis=1)  # Row sums\n",
    "    \n",
    "    print(f'Sum of tuples for each x-axis bucket (Column sums):')\n",
    "    for i, sum_val in enumerate(x_axis_sums):\n",
    "        print(f'Bucket {i}: {sum_val}')\n",
    "\n",
    "    print(f'Sum of tuples for each y-axis bucket (Row sums):')\n",
    "    for i, sum_val in enumerate(y_axis_sums):\n",
    "        print(f'Bucket {i}: {sum_val}')\n",
    "\n",
    "def plot_interactions(df, col1, col2, secondary1, secondary2, dataset_name, distribution_type, scale, bucket_suffix='_bucket', num_buckets=128):\n",
    "    # Determine the bucketed column names\n",
    "    bucket_col1 = col1 + bucket_suffix\n",
    "    bucket_col2 = col2 + bucket_suffix\n",
    "    \n",
    "    # Prepare figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # Plot Bucket Interactions\n",
    "    interaction_matrix = np.zeros((num_buckets, num_buckets), dtype=int)\n",
    "    interaction_counts = df.groupby([bucket_col1, bucket_col2]).size().reset_index(name='counts')\n",
    "    for _, row in interaction_counts.iterrows():\n",
    "        bucket1 = int(row[bucket_col1])\n",
    "        bucket2 = int(row[bucket_col2])\n",
    "        count = row['counts']\n",
    "        if bucket1 < num_buckets and bucket2 < num_buckets:\n",
    "            interaction_matrix[bucket1, bucket2] += count\n",
    "    \n",
    "    # Plot the heatmap\n",
    "    sns.heatmap(interaction_matrix, cmap='hot', annot=False, ax=ax1)\n",
    "    title1 = f'Bucket Heatmap between {col1 + \" + \" + secondary1 if secondary1 else col1} and {col2 + \" + \" + secondary2 if secondary2 else col2}'\n",
    "    ax1.set_title(title1, pad=12)\n",
    "    ax1.set_ylabel(bucket_col1)\n",
    "    ax1.set_xlabel(bucket_col2)\n",
    "    \n",
    "    # sanity_check_row_and_col_buckets(interaction_matrix)\n",
    "\n",
    "    # Plot Value Interactions\n",
    "    interaction_matrix = pd.crosstab(df[col1], df[col2]).values\n",
    "    sns.heatmap(interaction_matrix, cmap='hot', annot=False, ax=ax2)\n",
    "    title2 = f'Value Heatmap between {col1 + \" + \" + secondary1 if secondary1 else col1} and {col2 + \" + \" + secondary2 if secondary2 else col2}'\n",
    "    ax2.set_title(title2, pad=12)\n",
    "    ax2.set_ylabel(col1)\n",
    "    ax2.set_xlabel(col2)\n",
    "\n",
    "    # Add a main title for the page\n",
    "    main_title = f'Plots of {dataset_name} dataset showing {distribution_type} distribution in {scale} scale'\n",
    "    fig.suptitle(main_title, fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    return fig\n",
    "\n",
    "# Creating the PDF with plots for each pair of columns\n",
    "def generate_interaction_plots(df, columns, secondary_columns, dataset_name, distribution_type, scale, num_buckets=128):\n",
    "    with PdfPages('taxi_freq_minmax_sec.pdf') as pdf:\n",
    "        for i in range(len(columns)):\n",
    "            for j in range(i + 1, len(columns)):\n",
    "                col1 = columns[i]\n",
    "                col2 = columns[j]\n",
    "                sec1 = secondary_columns[i]\n",
    "                sec2 = secondary_columns[j]\n",
    "                \n",
    "                fig = plot_interactions(df, col1, col2, sec1, sec2, dataset_name, distribution_type, scale, num_buckets=num_buckets)\n",
    "                pdf.savefig(fig)\n",
    "                plt.close(fig)\n",
    "\n",
    "# Usage example assuming secondary_columns directly contains the names or None\n",
    "columns = ['passenger_count', 'trip_distance', 'PULocationID', 'DOLocationID', 'payment_type']\n",
    "secondary_columns = ['pickup_time', None, None, None, 'dropoff_time']\n",
    "# secondary_columns = [None, None, None, None, None]\n",
    "dataset_name = \"Taxi\"\n",
    "distribution_type = \"frequency\"\n",
    "scale = \"absolute\"\n",
    "generate_interaction_plots(df, columns, secondary_columns, dataset_name, distribution_type, scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.colors import TwoSlopeNorm, LinearSegmentedColormap\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "def plot_interactions_ideal(df, col1, col2, secondary1, secondary2, dataset_name, distribution_type, scale, bucket_suffix='_bucket', num_buckets=128):\n",
    "    # Determine the bucketed column names\n",
    "    bucket_col1 = col1 + bucket_suffix\n",
    "    bucket_col2 = col2 + bucket_suffix\n",
    "    \n",
    "    # Prepare figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # Plot Bucket Interactions\n",
    "    interaction_matrix = np.zeros((num_buckets, num_buckets), dtype=int)\n",
    "    interaction_counts = df.groupby([bucket_col1, bucket_col2]).size().reset_index(name='counts')\n",
    "    for _, row in interaction_counts.iterrows():\n",
    "        bucket1 = int(row[bucket_col1])\n",
    "        bucket2 = int(row[bucket_col2])\n",
    "        count = row['counts']\n",
    "        if bucket1 < num_buckets and bucket2 < num_buckets:\n",
    "            interaction_matrix[bucket1, bucket2] += count\n",
    "\n",
    "    # Calculate average frequency per bucket\n",
    "    total_counts = interaction_matrix.sum()\n",
    "    average_frequency_per_bucket = total_counts / (num_buckets ** 2)\n",
    "\n",
    "    # Custom colormap: green to black to red\n",
    "    cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", [\"green\", \"black\", \"red\"])\n",
    "    norm = TwoSlopeNorm(vmin=0, vcenter=average_frequency_per_bucket, vmax=2 * average_frequency_per_bucket)\n",
    "\n",
    "    sns.heatmap(interaction_matrix, cmap=cmap, norm=norm, annot=False, ax=ax1)\n",
    "    title1 = f'Bucket Heatmap between {col1 + \" + \" + secondary1 if secondary1 else col1} and {col2 + \" + \" + secondary2 if secondary2 else col2}'\n",
    "    ax1.set_title(title1, pad=12)\n",
    "    ax1.set_ylabel(bucket_col1)\n",
    "    ax1.set_xlabel(bucket_col2)\n",
    "\n",
    "    # Plot Value Interactions\n",
    "    interaction_matrix = pd.crosstab(df[col1], df[col2]).values\n",
    "    \n",
    "    # Calculate average frequency per value pair\n",
    "    unique_values_col1 = df[col1].unique()\n",
    "    unique_values_col2 = df[col2].unique()\n",
    "    total_counts_values = interaction_matrix.sum()\n",
    "    average_frequency_per_value_pair = total_counts_values / (len(unique_values_col1) * len(unique_values_col2))\n",
    "\n",
    "    norm = TwoSlopeNorm(vmin=0, vcenter=average_frequency_per_value_pair, vmax=2 * average_frequency_per_value_pair)\n",
    "\n",
    "    sns.heatmap(interaction_matrix, cmap=cmap, norm=norm, annot=False, ax=ax2)\n",
    "    title2 = f'Value Heatmap between {col1 + \" + \" + secondary1 if secondary1 else col1} and {col2 + \" + \" + secondary2 if secondary2 else col2}'\n",
    "    ax2.set_title(title2, pad=12)\n",
    "    ax2.set_ylabel(col1)\n",
    "    ax2.set_xlabel(col2)\n",
    "\n",
    "    # Add a main title for the page\n",
    "    main_title = f'Plots of {dataset_name} dataset showing {distribution_type} distribution in {scale} scale'\n",
    "    fig.suptitle(main_title, fontsize=16)\n",
    "    # Print the average frequency at the bottom of the page\n",
    "    fig.text(0.5, 0.01, f'Average Frequency per Bucket: {average_frequency_per_bucket:.2f}', ha='center', fontsize=12)   \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.93])  \n",
    "    return fig\n",
    "\n",
    "# Creating the PDF with plots for each pair of columns\n",
    "def generate_interaction_plots_ideal(df, columns, secondary_columns, dataset_name, distribution_type, scale, num_buckets=128):\n",
    "    with PdfPages('taxi_freq_ideal_sec.pdf') as pdf:\n",
    "        for i in range(len(columns)):\n",
    "            for j in range(i + 1, len(columns)):\n",
    "                col1 = columns[i]\n",
    "                col2 = columns[j]\n",
    "                sec1 = secondary_columns[i] if secondary_columns[i] != None else None\n",
    "                sec2 = secondary_columns[j] if secondary_columns[j] != None else None\n",
    "                \n",
    "                fig = plot_interactions_ideal(df, col1, col2, sec1, sec2, dataset_name, distribution_type, scale, num_buckets=num_buckets)\n",
    "                pdf.savefig(fig)\n",
    "                plt.close(fig)\n",
    "\n",
    "# Usage example assuming secondary_columns directly contains the names or None\n",
    "columns = ['passenger_count', 'trip_distance', 'PULocationID', 'DOLocationID', 'payment_type']\n",
    "# secondary_columns = [None, None, None, None, None]\n",
    "secondary_columns = ['pickup_time', None, None, None, 'dropoff_time']\n",
    "dataset_name = \"Taxi\"\n",
    "distribution_type = \"frequency\"\n",
    "scale = \"normalized\"\n",
    "generate_interaction_plots_ideal(df, columns, secondary_columns, dataset_name, distribution_type, scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "def sanity_check_row_and_col_buckets(interaction_matrix):\n",
    "    # Print column sums and row sums of the interaction matrix\n",
    "    x_axis_sums = interaction_matrix.sum(axis=0)  # Column sums\n",
    "    y_axis_sums = interaction_matrix.sum(axis=1)  # Row sums\n",
    "    \n",
    "    print(f'Sum of fares for each x-axis bucket (Column sums):')\n",
    "    for i, sum_val in enumerate(x_axis_sums):\n",
    "        print(f'Bucket {i}: {sum_val}')\n",
    "\n",
    "    print(f'Sum of fares for each y-axis bucket (Row sums):')\n",
    "    for i, sum_val in enumerate(y_axis_sums):\n",
    "        print(f'Bucket {i}: {sum_val}')\n",
    "\n",
    "def plot_interactions_fare(df, col1, col2, secondary1, secondary2, dataset_name, distribution_type, scale, bucket_suffix='_bucket', num_buckets=128):\n",
    "    # Determine the bucketed column names\n",
    "    bucket_col1 = col1 + bucket_suffix\n",
    "    bucket_col2 = col2 + bucket_suffix\n",
    "    \n",
    "    # Prepare figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # Plot Bucket Interactions (Summing up fare_amount)\n",
    "    interaction_matrix = np.zeros((num_buckets, num_buckets), dtype=float)\n",
    "    interaction_fares = df.groupby([bucket_col1, bucket_col2])['fare_amount'].sum().reset_index(name='total_fare')\n",
    "    for _, row in interaction_fares.iterrows():\n",
    "        bucket1 = int(row[bucket_col1])\n",
    "        bucket2 = int(row[bucket_col2])\n",
    "        total_fare = row['total_fare']\n",
    "        if bucket1 < num_buckets and bucket2 < num_buckets:\n",
    "            interaction_matrix[bucket1, bucket2] += total_fare\n",
    "    \n",
    "    # Plot the heatmap\n",
    "    sns.heatmap(interaction_matrix, cmap='hot', annot=False, ax=ax1)\n",
    "    title1 = f'Bucket Heatmap (Total Fare) between {col1 + \" + \" + secondary1 if secondary1 else col1} and {col2 + \" + \" + secondary2 if secondary2 else col2}'\n",
    "    ax1.set_title(title1, pad=12)\n",
    "    ax1.set_ylabel(bucket_col1)\n",
    "    ax1.set_xlabel(bucket_col2)\n",
    "    \n",
    "    # sanity_check_row_and_col_buckets(interaction_matrix)\n",
    "\n",
    "    # Plot Value Interactions (Summing up fare_amount)\n",
    "    unique_values_col1 = df[col1].unique()\n",
    "    unique_values_col2 = df[col2].unique()\n",
    "    interaction_matrix = pd.DataFrame(index=unique_values_col1, columns=unique_values_col2, data=0.0)\n",
    "    interaction_fares = df.groupby([col1, col2])['fare_amount'].sum().reset_index(name='total_fare')\n",
    "    for _, row in interaction_fares.iterrows():\n",
    "        value1 = row[col1]\n",
    "        value2 = row[col2]\n",
    "        total_fare = row['total_fare']\n",
    "        interaction_matrix.loc[value1, value2] = total_fare\n",
    "\n",
    "    # Mapping the unique values to indices\n",
    "    index_mapping_col1 = {val: idx+1 for idx, val in enumerate(unique_values_col1)}\n",
    "    index_mapping_col2 = {val: idx+1 for idx, val in enumerate(unique_values_col2)}\n",
    "\n",
    "    # Create a new DataFrame for the interaction matrix with indices\n",
    "    interaction_matrix_indexed = interaction_matrix.rename(index=index_mapping_col1, columns=index_mapping_col2)\n",
    "\n",
    "    sns.heatmap(interaction_matrix_indexed, cmap='hot', annot=False, ax=ax2)\n",
    "    title2 = f'Value Heatmap (Total Fare) between {col1 + \" + \" + secondary1 if secondary1 else col1} and {col2 + \" + \" + secondary2 if secondary2 else col2}'\n",
    "    ax2.set_title(title2, pad=12)\n",
    "    ax2.set_ylabel(f'{col1} (Indices)')\n",
    "    ax2.set_xlabel(f'{col2} (Indices)')\n",
    "\n",
    "    # Add a main title for the page\n",
    "    main_title = f'Plots of {dataset_name} dataset showing {distribution_type} distribution in {scale} scale'\n",
    "    fig.suptitle(main_title, fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    return fig\n",
    "\n",
    "# Creating the PDF with plots for each pair of columns\n",
    "def generate_interaction_plots_fare(df, columns, secondary_columns, dataset_name, distribution_type, scale, num_buckets=128):\n",
    "    with PdfPages('taxi_price_minmax_sec.pdf') as pdf:\n",
    "        for i in range(len(columns)):\n",
    "            for j in range(i + 1, len(columns)):\n",
    "                col1 = columns[i]\n",
    "                col2 = columns[j]\n",
    "                sec1 = secondary_columns[i]\n",
    "                sec2 = secondary_columns[j]\n",
    "                \n",
    "                fig = plot_interactions_fare(df, col1, col2, sec1, sec2, dataset_name, distribution_type, scale, num_buckets=num_buckets)\n",
    "                pdf.savefig(fig)\n",
    "                plt.close(fig)\n",
    "\n",
    "# Usage example assuming secondary_columns directly contains the names or None\n",
    "columns = ['passenger_count', 'trip_distance', 'PULocationID', 'DOLocationID', 'payment_type']\n",
    "secondary_columns = ['pickup_time', None, None, None, 'dropoff_time']\n",
    "# secondary_columns = [None, None, None, None, None]\n",
    "dataset_name = \"Taxi\"\n",
    "distribution_type = \"fare\"\n",
    "scale = \"Absolute\"\n",
    "generate_interaction_plots_fare(df, columns, secondary_columns, dataset_name, distribution_type, scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.colors import TwoSlopeNorm, LinearSegmentedColormap\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "def plot_interactions_fare_ideal(df, col1, col2, secondary1, secondary2, dataset_name, distribution_type, scale, bucket_suffix='_bucket', num_buckets=128):\n",
    "    # Determine the bucketed column names\n",
    "    bucket_col1 = col1 + bucket_suffix\n",
    "    bucket_col2 = col2 + bucket_suffix\n",
    "    \n",
    "    # Prepare figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # Plot Bucket Interactions (Summing up fare_amount)\n",
    "    interaction_matrix = np.zeros((num_buckets, num_buckets), dtype=float)\n",
    "    interaction_fares = df.groupby([bucket_col1, bucket_col2])['fare_amount'].sum().reset_index(name='total_fare')\n",
    "    for _, row in interaction_fares.iterrows():\n",
    "        bucket1 = int(row[bucket_col1])\n",
    "        bucket2 = int(row[bucket_col2])\n",
    "        total_fare = row['total_fare']\n",
    "        if bucket1 < num_buckets and bucket2 < num_buckets:\n",
    "            interaction_matrix[bucket1, bucket2] += total_fare\n",
    "\n",
    "    # Total fares to determine average frequency for normalization\n",
    "    total_fares = interaction_matrix.sum()\n",
    "    average_fare_buckets = total_fares / (num_buckets ** 2)\n",
    "\n",
    "    # Custom colormap: green to black to red\n",
    "    cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", [\"green\", \"black\", \"red\"])\n",
    "    norm = TwoSlopeNorm(vmin=0, vcenter=average_fare_buckets, vmax=2*average_fare_buckets)\n",
    "\n",
    "    sns.heatmap(interaction_matrix, cmap=cmap, norm=norm, annot=False, ax=ax1)\n",
    "    title1 = f'Bucket Heatmap (Total Fare) between {col1 + \" + \" + secondary1 if secondary1 else col1} and {col2 + \" + \" + secondary2 if secondary2 else col2}'\n",
    "    ax1.set_title(title1, pad=12)\n",
    "    ax1.set_ylabel(bucket_col1)\n",
    "    ax1.set_xlabel(bucket_col2)\n",
    "\n",
    "    # Plot Value Interactions (Summing up fare_amount)\n",
    "    unique_values_col1 = df[col1].unique()\n",
    "    unique_values_col2 = df[col2].unique()\n",
    "    interaction_matrix = pd.DataFrame(index=unique_values_col1, columns=unique_values_col2, data=0.0)\n",
    "    interaction_fares = df.groupby([col1, col2])['fare_amount'].sum().reset_index(name='total_fare')\n",
    "    for _, row in interaction_fares.iterrows():\n",
    "        value1 = row[col1]\n",
    "        value2 = row[col2]\n",
    "        total_fare = row['total_fare']\n",
    "        interaction_matrix.loc[value1, value2] = total_fare\n",
    "\n",
    "    # Mapping the unique values to indices\n",
    "    index_mapping_col1 = {val: idx+1 for idx, val in enumerate(unique_values_col1)}\n",
    "    index_mapping_col2 = {val: idx+1 for idx, val in enumerate(unique_values_col2)}\n",
    "\n",
    "    # Create a new DataFrame for the interaction matrix with indices\n",
    "    interaction_matrix_indexed = interaction_matrix.rename(index=index_mapping_col1, columns=index_mapping_col2)\n",
    "\n",
    "    # Total fares to determine average frequency for normalization\n",
    "    total_fares_values = interaction_matrix_indexed.values.sum()\n",
    "    average_fare_values = total_fares_values / (len(unique_values_col1) * len(unique_values_col2))\n",
    "\n",
    "    norm = TwoSlopeNorm(vmin=0, vcenter=average_fare_values, vmax=2*average_fare_values)\n",
    "\n",
    "    sns.heatmap(interaction_matrix_indexed, cmap=cmap, norm=norm, annot=False, ax=ax2)\n",
    "    title2 = f'Value Interaction Heatmap (Total Fare) between {col1 + \" + \" + secondary1 if secondary1 else col1} and {col2 + \" + \" + secondary2 if secondary2 else col2}'\n",
    "    ax2.set_title(title2, pad=12)\n",
    "    ax2.set_ylabel(f'{col1} (Indices)')\n",
    "    ax2.set_xlabel(f'{col2} (Indices)')\n",
    "    \n",
    "    # Add a main title for the page\n",
    "    main_title = f'Plots of {dataset_name} dataset showing {distribution_type} distribution in {scale} scale'\n",
    "    fig.suptitle(main_title, fontsize=16)\n",
    "    # Print the average price at the bottom of the page\n",
    "    fig.text(0.5, 0.01, f'Average Fare per Bucket: {average_fare_buckets:.2f}', ha='center', fontsize=12)  \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])  \n",
    "    return fig\n",
    "\n",
    "# Creating the PDF with plots for each pair of columns\n",
    "def generate_interaction_plots_fare_ideal(df, columns, secondary_columns, dataset_name, distribution_type, scale, num_buckets=128):\n",
    "    with PdfPages('taxi_price_ideal_sec.pdf') as pdf:\n",
    "        for i in range(len(columns)):\n",
    "            for j in range(i + 1, len(columns)):\n",
    "                col1 = columns[i]\n",
    "                col2 = columns[j]\n",
    "                sec1 = secondary_columns[i] if secondary_columns[i] != None else None\n",
    "                sec2 = secondary_columns[j] if secondary_columns[j] != None else None\n",
    "                \n",
    "                fig = plot_interactions_fare_ideal(df, col1, col2, sec1, sec2, dataset_name, distribution_type, scale, num_buckets=num_buckets)\n",
    "                pdf.savefig(fig)\n",
    "                plt.close(fig)\n",
    "\n",
    "# Usage example assuming secondary_columns directly contains the names or None\n",
    "columns = ['passenger_count', 'trip_distance', 'PULocationID', 'DOLocationID', 'payment_type']\n",
    "secondary_columns = ['pickup_time', None, None, None, 'dropoff_time']\n",
    "# secondary_columns = [None, None, None, None, None]\n",
    "dataset_name = \"Taxi\"\n",
    "distribution_type = \"fare\"\n",
    "scale = \"Normalized\"\n",
    "\n",
    "generate_interaction_plots_fare_ideal(df, columns, secondary_columns, dataset_name, distribution_type, scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
